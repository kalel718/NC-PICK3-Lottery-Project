{\rtf1\ansi\ansicpg1252\cocoartf2513
\cocoatextscaling0\cocoaplatform0{\fonttbl\f0\fswiss\fcharset0 Helvetica;\f1\fnil\fcharset0 AppleColorEmoji;}
{\colortbl;\red255\green255\blue255;}
{\*\expandedcolortbl;;}
\margl1440\margr1440\vieww10800\viewh8400\viewkind0
\pard\tx720\tx1440\tx2160\tx2880\tx3600\tx4320\tx5040\tx5760\tx6480\tx7200\tx7920\tx8640\pardirnatural\partightenfactor0

\f0\fs24 \cf0 # 
\f1 \uc0\u55357 \u56693 \u65039 \u8205 \u9794 \u65039 
\f0  Data Forensics: Pick 3 Pattern Analysis\
### [cite_start]CASE FILE: DATA_MASTERY_01 // STATUS: OPEN [cite: 39]\
\
## Project Overview\
I\'92ve always been a fan of how the lottery works\'97**Pick 3** is my go-to. I decided to use technology to test out some theories and have some fun with the numbers. This project uses Python and Pandas to transform raw, messy lottery data into structured insights. \
\
**Note:** This project is for **educational purposes only**. It\'92s a study on how Python can handle "noisy" data and identify historical patterns.\
\
---\
\
## [cite_start]
\f1 \uc0\u55357 \u57056 
\f0  The Toolkit [cite: 46]\
* [cite_start]**Language:** Python [cite: 41]\
* [cite_start]**Library:** Pandas (The industry-standard toolbox for data manipulation) [cite: 42, 46]\
* [cite_start]**Platform:** Google Colab [cite: 43]\
\
---\
\
## 
\f1 \uc0\u55357 \u56589 
\f0  The Investigation Process\
\
### 1. Opening the Vault (Data Loading)\
[cite_start]The investigation begins by importing Pandas and bridging the gap between local storage and the cloud[cite: 46]. [cite_start]We transform raw text into a structured DataFrame (`df`) and use `head()` to take a sneak peek at the evidence[cite: 57].\
* [cite_start]`df = pd.read_csv('your_filename_here.csv')` [cite: 53]\
\
### 2. Removing the Noise\
[cite_start]Not all data is a signal[cite: 70]. [cite_start]I discarded irrelevant columns (the "red herrings") like `GreenBall` or `DoubleDraw` to focus purely on the core numbers[cite: 68, 69, 71].\
* [cite_start]`df.drop(columns=columns_to_remove, axis=1, inplace=True)` [cite: 63, 64]\
\
### 3. The Deep Clean (Data Standardization)\
[cite_start]Messy input like "Error" strings or missing values (NaNs) are potholes that can crash calculation scripts[cite: 99, 107]. \
* [cite_start]**Standardization:** Iterating through columns and forcing inconsistent decimals into clean integers[cite: 81].\
* [cite_start]**Coercion:** Using `pd.to_numeric` with `errors='coerce'` to force non-numeric garbage into a "Safe State" (NaN)[cite: 106, 109].\
* [cite_start]**Patching:** Strategically filling voids with `0` to maintain structural integrity[cite: 99].\
* [cite_start]`df['Fireball'] = df['Fireball'].fillna(0)` [cite: 96]\
\
### 4. Row-Wise Sorting (The Magic Line)\
[cite_start]In a "Box" win, **1-5-9** and **9-1-5** are the same result, but computers see them as different[cite: 150]. [cite_start]I applied a **Lambda function** to sort numbers *across* the row to normalize the chaos[cite: 151].\
* [cite_start]`.apply(lambda row: sorted(row.values))` [cite: 145]\
\
### [cite_start]5. Frequency & Combinatorics [cite: 263]\
* [cite_start]**Frequency Analysis:** Flattening the dataset using `.stack()` to count the total frequency of digits 0-9[cite: 161].\
* [cite_start]**Finding Pairs:** Using `itertools.combinations` to uncover hidden correlations and identify which numbers appear together as "accomplices" most often[cite: 241, 265].\
\
---\
\
## [cite_start]
\f1 \uc0\u55357 \u56514 
\f0  Securing the Evidence [cite: 289]\
[cite_start]Once the data is cleaned and the patterns are obvious, the cycle is complete: **Raw Data -> Clean -> Insight -> Saved Asset**[cite: 294].\
* [cite_start]`df.to_csv('cleaned_data.csv', index=False)` [cite: 286]\
\
> [cite_start]**Strategic Insight:** Python allows us to see patterns the human eye would miss by normalizing chaos[cite: 156].\
\
[cite_start]**Case Status: CLOSED** [cite: 298]}